# Go ETL Pipeline ğŸš€

## ğŸ“Œ Overview  
This project is a **Go-based ETL (Extract, Transform, Load) pipeline** that:  
âœ… Fetches user data from an **external API (`https://randomuser.me/api/`)** every 30 seconds  
âœ… Stores **raw JSON responses** in **PostgreSQL** and in local storage (`data/raw/`)  
âœ… Uses **Go channels & goroutines** for **parallel data processing**  
âœ… Transforms the data (extracts `name`, `email`, `dob`)  
âœ… Saves **processed data in JSON format (`data/processed/`)**  
âœ… Exposes a **health check endpoint** (`http://localhost:8080/health`)  

---

## ğŸ“‚ Project Structure  
```plaintext
ğŸ“‚ etl-pipeline
 â”œâ”€â”€ ğŸ“‚ config/                
 â”‚    â”œâ”€â”€ config.go           # Loads database & API configurations
 â”œâ”€â”€ ğŸ“‚ data/                    
 â”‚    â”œâ”€â”€ raw/                # Contain raw data
 â”‚    â”œâ”€â”€ processed/          # Contain processed data
 â”œâ”€â”€ ğŸ“‚ db/                    
 â”‚    â”œâ”€â”€ db.go               # Handles PostgreSQL connection
 â”‚    â”œâ”€â”€ init.sql            # Creates PostgreSQL tables on startup
 â”œâ”€â”€ ğŸ“‚ services/              
 â”‚    â”œâ”€â”€ fetch.go            # Fetches raw API data
 â”‚    â”œâ”€â”€ transform.go        # Transforms the data
 â”‚    â”œâ”€â”€ store.go            # Saves processed data
 â”œâ”€â”€ ğŸ“‚ utils/                 
 â”‚    â”œâ”€â”€ helper.go           # Helper utilities
 â”œâ”€â”€ ğŸ“‚ logs/                   # Contain log file  
 â”œâ”€â”€ main.go                   # Application entry point (goroutines & channels)
 â”œâ”€â”€ Dockerfile                # Docker containerization
 â”œâ”€â”€ docker-compose.yml        # Manages ETL & PostgreSQL containers
 â”œâ”€â”€ go.mod                    # Go dependencies
 â”œâ”€â”€ README.md                 # Documentation
```
## ğŸ“Œ Prerequisites

Before running the application, make sure you have:

- Go 1.18+ â†’ Install from golang.org
- Docker & Docker Compose â†’ Install from docker.com
- PostgreSQL â†’ Either install locally or use Docker

## Running the ETL Pipeline (Without Docker)

- Step 1: Configure Database
1ï¸âƒ£ Start PostgreSQL and create a database:
```
psql -U postgres -c "CREATE DATABASE etl_db;"
```
1ï¸âƒ£ Create the required table:
```
CREATE TABLE raw_data (
    id SERIAL PRIMARY KEY,
    data JSONB NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);
```
- Step 2: Setup Environment
Modify config/config.go to match your PostgreSQL credentials.

- Step 3: Run the Application
```
go run main.go
```
## Running with Docker
- Step 1: Build & Start the Containers
```
docker-compose up --build
```
This will:

âœ… Start PostgreSQL and automatically create the required tables

âœ… Start the Go ETL app, fetching and processing data every 30 seconds

- Step 2: Monitoring the ETL Pipeline

1. Check Running Containers
```
docker ps
```

2. Check Processed Data
```
cat data/processed/processed_data.json
```

3. Check service health
```
curl http://localhost:8080/health
```